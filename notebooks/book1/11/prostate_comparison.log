An error occurred while executing the following cell:
------------------
"""
Compares L1, L2, allSubsets, and OLS linear regression on the prostate data set
Author : Aleyna Kara (@karalleyna)
Based on https://github.com/probml/pmtk3/blob/master/demos/prostateComparison.m
Sourced from https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Prostate%20Cancer.ipynb
"""


try:
    import probml_utils as pml
except ModuleNotFoundError:
    %pip install -qq git+https://github.com/probml/probml-utils.git
    import probml_utils as pml
import numpy as np
import matplotlib.pyplot as plt

try:
    import pandas as pd
except ModuleNotFoundError:
    %pip install -qq pandas
    import pandas as pd
from itertools import combinations

try:
    from sklearn.linear_model import Ridge
except ModuleNotFoundError:
    %pip install -qq scikit-learn
    from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import LinearRegression

from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler


def get_features_and_label(dataset, is_training):
    """
    Gets matrices representing features and target from the original data set
    Parameters
    ----------
    dataset: DataFrame
      Original dataset
    is_training : str
      Label to show whether a data point is in training data or not.
        * "T" -> Training data
        * "F" -> Test data
    Return
    ------
      X : ndarray
        Feature matrix
      y : ndarray
        Lpsa values of each data point.
    """
    X = dataset.loc[dataset.train == is_training].drop("train", axis=1)
    y = X.pop("lpsa").values
    X = X.to_numpy()
    return X, y


class OneStandardErrorRuleModel:
    """
    Select the least complex model among one standard error of the best.
    Attributes
    ----------
        estimator :
            A regression model to be parametrized.
        params : dict
            * Keys : The parameter of the model to be chosen by cross-validation.
            * Values : The values for the parameter to be tried.
        cv : Int
            The number of folds for cross-validation.
    """

    def __init__(self, estimator, params, cv=10):
        self.estimator = estimator
        self.cv = cv
        self.params = params
        self.random_state = 69438  # Seed of the pseudo random number generator

    def fit(self, X, y):
        grid_search = GridSearchCV(
            self.estimator,
            self.params,
            cv=KFold(self.cv, shuffle=True, random_state=self.random_state),
            scoring="neg_mean_squared_error",
            return_train_score=True,
        )
        grid_search = grid_search.fit(X, y)
        # Gets best estimator according to one standard error rule model
        model_idx = self._get_best_estimator(grid_search.cv_results_)
        self.refit(X, y, model_idx)
        return self

    def _get_best_estimator(self, cv_results):
        cv_mean_errors = -cv_results["mean_test_score"]  # Mean errors
        cv_errors = -np.vstack([cv_results[f"split{i}_test_score"] for i in range(self.cv)]).T
        cv_mean_errors_std = np.std(cv_errors, ddof=1, axis=1) / np.sqrt(self.cv)  # Standard errors

        # Finds smallest mean and standard error
        cv_min_error, cv_min_error_std = self._get_cv_min_error(cv_mean_errors, cv_mean_errors_std)

        error_threshold = cv_min_error + cv_min_error_std
        # Finds the least complex model within one standard error of the best
        model_idx = np.argmax(cv_mean_errors < error_threshold)
        cv_mean_error_ = cv_mean_errors[model_idx]
        cv_mean_errors_std_ = cv_mean_errors_std[model_idx]
        return model_idx

    def _get_cv_min_error(self, cv_mean_errors, cv_mean_errors_std):
        # Gets the index of the model with minimum mean error
        best_model_idx = np.argmin(cv_mean_errors)
        cv_min_error = cv_mean_errors[best_model_idx]
        cv_min_error_std = cv_mean_errors_std[best_model_idx]
        return cv_min_error, cv_min_error_std

    def refit(self, X, y, model_idx):
        if self.params:
            param_name = list(self.params.keys())[0]
            self.estimator.set_params(**{param_name: self.params[param_name][model_idx]})
        # Fits the selected model
        self.estimator.fit(X, y)

    def get_test_scores(self, y_test, y_pred):
        y_test, y_pred = y_test.reshape((1, -1)), y_pred.reshape((1, -1))
        errors = (y_test - y_pred) ** 2  # Least sqaure errors
        error = np.mean(errors)  # Mean least sqaure errors
        error_std = np.std(errors, ddof=1) / np.sqrt(y_test.size)  # Standard errors
        return error, error_std


class BestSubsetRegression(LinearRegression):
    """
    Linear regression based on the best features subset of fixed size.
    Attributes
    ----------
        subset_size : Int
            The number of features in the subset.
    """

    def __init__(self, subset_size=1):
        LinearRegression.__init__(self)
        self.subset_size = subset_size

    def fit(self, X, y):
        best_combination, best_mse = None, np.inf
        best_intercept_, best_coef_ = None, None
        # Tries all combinations of subset_size
        for combination in combinations(range(X.shape[1]), self.subset_size):
            X_subset = X[:, combination]
            LinearRegression.fit(self, X_subset, y)
            mse = mean_squared_error(y, self.predict(X_subset))
            # Updates the best combination if it gives better result than the current best
            if best_mse > mse:
                best_combination, best_mse = combination, mse
                best_intercept_, best_coef_ = self.intercept_, self.coef_
        LinearRegression.fit(self, X, y)
        # Sets intercept and parameters
        self.intercept_ = best_intercept_
        self.coef_[:] = 0
        self.coef_[list(best_combination)] = best_coef_
        return self


path = "https://raw.githubusercontent.com/probml/probml-data/main/data/prostate/prostate.csv"
X = pd.read_csv(path, sep="\t").iloc[:, 1:]
X_train, y_train = get_features_and_label(X, "T")
X_test, y_test = get_features_and_label(X, "F")

# Standardizes training and test data
scaler = StandardScaler().fit(X.loc[:, "lcavol":"pgg45"])
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

n_models, cv, n_alphas = 4, 10, 30
_, n_features = X_train.shape
alpha_lasso, alpha_ridge = [0.680, 0.380, 0.209, 0.100, 0.044, 0.027, 0.012, 0.001], [
    436,
    165,
    82,
    44,
    27,
    12,
    4,
    1e-05,
]

linear_regression = OneStandardErrorRuleModel(LinearRegression(), {}).fit(X_train, y_train)
bs_regression = OneStandardErrorRuleModel(BestSubsetRegression(), {"subset_size": list(range(1, 9))}).fit(
    X_train, y_train
)
ridge_regression = OneStandardErrorRuleModel(Ridge(), {"alpha": alpha_ridge}).fit(X_train, y_train)
lasso_regression = OneStandardErrorRuleModel(Lasso(), {"alpha": alpha_lasso}).fit(X_train, y_train)

regressions = [linear_regression, bs_regression, ridge_regression, lasso_regression]

residuals = np.zeros((X_test.shape[0], n_models))  # (num of test data) x num of models
table = np.zeros(
    (n_features + 3, n_models)
)  # (num of features + 1(mean error) + 1(std error) + 1(bias coef)) x num of models

for i in range(n_models):
    table[:, i] = regressions[i].estimator.intercept_  # bias
    table[1 : n_features + 1, i] = regressions[i].estimator.coef_
    y_pred = regressions[i].estimator.predict(X_test)
    table[n_features + 1 :, i] = np.r_[regressions[i].get_test_scores(y_test, y_pred)]
    residuals[:, i] = np.abs(y_test - y_pred)

xlabels = ["Term", "LS", "Best Subset", "Ridge", "Lasso"]  # column headers
row_labels = np.r_[
    [["Intercept"]], X.columns[:-2].to_numpy().reshape(-1, 1), [["Test Error"], ["Std Error"]]
]  # row headers
row_values = np.c_[row_labels, np.round(table, 3)]

fig = plt.figure(figsize=(10, 4))
ax = plt.gca()

fig.patch.set_visible(False)
ax.axis("off")
ax.axis("tight")

table = ax.table(cellText=row_values, colLabels=xlabels, loc="center", cellLoc="center")
table.set_fontsize(20)
table.scale(1.5, 1.5)
fig.tight_layout()
pml.savefig("prostate-subsets-coef.pdf")
plt.show()

plt.figure()
plt.boxplot(residuals)
plt.xticks(np.arange(n_models) + 1, xlabels[1:])
pml.savefig("prostate-subsets-CV.pdf")
plt.show()
------------------

---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
/tmp/ipykernel_4124/4157790482.py in <module>
    164 
    165 path = "https://raw.githubusercontent.com/probml/probml-data/main/data/prostate/prostate.csv"
--> 166 X = pd.read_csv(path, sep="\t").iloc[:, 1:]
    167 X_train, y_train = get_features_and_label(X, "T")
    168 X_test, y_test = get_features_and_label(X, "F")

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    309                     stacklevel=stacklevel,
    310                 )
--> 311             return func(*args, **kwargs)
    312 
    313         return wrapper

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    584     kwds.update(kwds_defaults)
    585 
--> 586     return _read(filepath_or_buffer, kwds)
    587 
    588 

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)
    480 
    481     # Create the parser.
--> 482     parser = TextFileReader(filepath_or_buffer, **kwds)
    483 
    484     if chunksize or iterator:

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)
    809             self.options["has_index_names"] = kwds["has_index_names"]
    810 
--> 811         self._engine = self._make_engine(self.engine)
    812 
    813     def close(self):

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/readers.py in _make_engine(self, engine)
   1038             )
   1039         # error: Too many arguments for "ParserBase"
-> 1040         return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
   1041 
   1042     def _failover_to_python(self):

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py in __init__(self, src, **kwds)
     49 
     50         # open handles
---> 51         self._open_handles(src, kwds)
     52         assert self.handles is not None
     53 

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py in _open_handles(self, src, kwds)
    227             memory_map=kwds.get("memory_map", False),
    228             storage_options=kwds.get("storage_options", None),
--> 229             errors=kwds.get("encoding_errors", "strict"),
    230         )
    231 

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
    612         compression=compression,
    613         mode=mode,
--> 614         storage_options=storage_options,
    615     )
    616 

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/common.py in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)
    310         # assuming storage_options is to be interpreted as headers
    311         req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
--> 312         with urlopen(req_info) as req:
    313             content_encoding = req.headers.get("Content-Encoding", None)
    314             if content_encoding == "gzip":

~/miniconda3/envs/py37/lib/python3.7/site-packages/pandas/io/common.py in urlopen(*args, **kwargs)
    210     import urllib.request
    211 
--> 212     return urllib.request.urlopen(*args, **kwargs)
    213 
    214 

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    220     else:
    221         opener = _opener
--> 222     return opener.open(url, data, timeout)
    223 
    224 def install_opener(opener):

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in open(self, fullurl, data, timeout)
    529         for processor in self.process_response.get(protocol, []):
    530             meth = getattr(processor, meth_name)
--> 531             response = meth(req, response)
    532 
    533         return response

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in http_response(self, request, response)
    639         if not (200 <= code < 300):
    640             response = self.parent.error(
--> 641                 'http', request, response, code, msg, hdrs)
    642 
    643         return response

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in error(self, proto, *args)
    567         if http_err:
    568             args = (dict, 'default', 'http_error_default') + orig_args
--> 569             return self._call_chain(*args)
    570 
    571 # XXX probably also want an abstract factory that knows when it makes

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in _call_chain(self, chain, kind, meth_name, *args)
    501         for handler in handlers:
    502             func = getattr(handler, meth_name)
--> 503             result = func(*args)
    504             if result is not None:
    505                 return result

~/miniconda3/envs/py37/lib/python3.7/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs)
    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--> 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 503: first byte timeout
HTTPError: HTTP Error 503: first byte timeout
