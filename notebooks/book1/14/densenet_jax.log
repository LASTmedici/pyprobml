An error occurred while executing the following cell:
------------------
timer = Timer()
animator = Animator(xlabel="epoch", xlim=[1, num_epochs], legend=["train loss", "train acc", "test acc"])
num_batches = len(train_iter)
device = torch.device(f"cuda:{0}")

for epoch in range(num_epochs):
    # Sum of training loss, sum of training accuracy, no. of examples
    metric = Accumulator(3)
    for i, (X, y) in enumerate(train_iter):
        timer.start()
        batch = {}
        batch["image"] = jnp.reshape(jnp.float32(X), (-1, 96, 96, 1))
        batch["label"] = jnp.float32(y)
        state, metrics = train_step(state, batch)
        metric.add(metrics["loss"] * X.shape[0], metrics["numcorrect"], X.shape[0])
        timer.stop()
        train_l = metric[0] / metric[2]
        train_acc = metric[1] / metric[2]
        if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
            animator.add(epoch + (i + 1) / num_batches, (train_l, train_acc, None))

    test_acc = eval_model(state, test_iter)
    animator.add(epoch + 1, (None, None, test_acc))


print(f"{metric[2] * num_epochs / timer.sum():.1f} examples/sec " f"on {str(device)}")
print(f"loss {train_l:.3f}, train acc {train_acc:.3f}, " f"test acc {test_acc:.3f}")
------------------

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_2888/1115125.py in <module>
     12         batch["image"] = jnp.reshape(jnp.float32(X), (-1, 96, 96, 1))
     13         batch["label"] = jnp.float32(y)
---> 14         state, metrics = train_step(state, batch)
     15         metric.add(metrics["loss"] * X.shape[0], metrics["numcorrect"], X.shape[0])
     16         timer.stop()

    [... skipping hidden 14 frame]

/tmp/ipykernel_2888/1580938479.py in train_step(state, batch)
     13 
     14     grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
---> 15     aux, grads = grad_fn(state.params)
     16     # grads = lax.pmean(grads, axis_name='batch')
     17 

    [... skipping hidden 8 frame]

/tmp/ipykernel_2888/1580938479.py in loss_fn(params)
      5     def loss_fn(params):
      6         logits, new_model_state = state.apply_fn(
----> 7             {"params": params, "batch_stats": state.batch_stats}, batch["image"], mutable=["batch_stats"]
      8         )
      9         one_hot = jax.nn.one_hot(batch["label"], num_classes=10)

    [... skipping hidden 7 frame]

/tmp/ipykernel_2888/2176403876.py in __call__(self, x, train)
     21             # A transition layer that halves the number of channels is added between the dense blocks.
     22             if i != len(num_convs_in_dense_blocks) - 1:
---> 23                 x = TransitionBlock(num_channels // 2, norm)(x)
     24                 num_channels = num_channels // 2
     25 

    [... skipping hidden 3 frame]

/tmp/ipykernel_2888/585409295.py in __call__(self, x)
      7         x = self.norm()(x)
      8         x = nn.relu(x)
----> 9         x = nn.Conv(self.filters, (1, 1), padding=[(0, 0), (0, 0)], dtype=jnp.float32)(x)
     10         x = nn.avg_pool(x, (2, 2), (2, 2), padding=[(0, 0), (0, 0)])
     11         return x

    [... skipping hidden 3 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/flax/linen/linear.py in __call__(self, inputs)
    383 
    384     kernel = self.param('kernel', self.kernel_init, kernel_shape,
--> 385                         self.param_dtype)
    386     kernel = jnp.asarray(kernel, self.dtype)
    387 

    [... skipping hidden 10 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/nn/initializers.py in init(key, shape, dtype)
    229         # constant is stddev of standard normal truncated to (-2, 2)
    230         stddev = jnp.sqrt(variance) / jnp.array(.87962566103423978, dtype)
--> 231         return random.truncated_normal(key, -2, 2, shape, dtype) * stddev
    232       else:
    233         # constant is stddev of complex standard normal truncated to 2

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/random.py in truncated_normal(key, lower, upper, shape, dtype)
    655   if shape is not None:
    656     shape = core.as_named_shape(shape)
--> 657   return _truncated_normal(key, lower, upper, shape, dtype)  # type: ignore
    658 
    659 @partial(jit, static_argnums=(3, 4), inline=True)

    [... skipping hidden 7 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/random.py in _truncated_normal(key, lower, upper, shape, dtype)
    671   if not jnp.issubdtype(dtype, np.floating):
    672     raise TypeError("truncated_normal only accepts floating point dtypes.")
--> 673   u = uniform(key, shape, dtype, minval=a, maxval=b)
    674   out = sqrt2 * lax.erf_inv(u)
    675   # Clamp the value to the open interval (lower, upper) to make sure that

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/random.py in uniform(key, shape, dtype, minval, maxval)
    235   dtype = dtypes.canonicalize_dtype(dtype)
    236   shape = core.as_named_shape(shape)
--> 237   return _uniform(key, shape, dtype, minval, maxval)  # type: ignore
    238 
    239 @partial(jit, static_argnums=(1, 2), inline=True)

    [... skipping hidden 7 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/random.py in _uniform(key, shape, dtype, minval, maxval)
    254     raise TypeError("uniform only accepts 32- or 64-bit dtypes.")
    255 
--> 256   bits = _random_bits(key, nbits, shape)
    257 
    258   # The strategy here is to randomize only the mantissa bits with an exponent of

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/random.py in _random_bits(key, bit_width, shape)
     84 def _random_bits(key: prng.PRNGKeyArray, bit_width, shape) -> jnp.ndarray:
     85   key, _ = _check_prng_key(key)
---> 86   return key._random_bits(bit_width, shape)
     87 
     88 

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/prng.py in _random_bits(self, bit_width, shape)
    198 
    199   def _random_bits(self, bit_width, shape) -> jnp.ndarray:
--> 200     return self.impl.random_bits(self._keys, bit_width, shape)
    201 
    202   def _split(self, num: int) -> 'PRNGKeyArray':

    [... skipping hidden 7 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/prng.py in threefry_random_bits(key, bit_width, shape)
    516 
    517   if not nblocks:
--> 518     bits = threefry_2x32(key, lax.iota(np.uint32, rem))
    519   else:
    520     keys = threefry_split(key, nblocks + 1)

    [... skipping hidden 7 frame]

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/prng.py in threefry_2x32(keypair, count)
    463     x = list(jnp.split(jnp.concatenate([count.ravel(), np.uint32([0])]), 2))
    464   else:
--> 465     x = list(jnp.split(count.ravel(), 2))
    466 
    467   x = threefry2x32_p.bind(key1, key2, x[0], x[1])

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py in split(ary, indices_or_sections, axis)
   1094 @_wraps(np.split, lax_description=_ARRAY_VIEW_DOC)
   1095 def split(ary, indices_or_sections, axis: int = 0):
-> 1096   return _split("split", ary, indices_or_sections, axis=axis)
   1097 
   1098 def _split_on_axis(op, axis):

~/miniconda3/envs/py37/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py in _split(op, ary, indices_or_sections, axis)
   1090   _subval = lambda x, i, v: subvals(x, [(i, v)])
   1091   return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
-> 1092           for start, end in zip(split_indices[:-1], split_indices[1:])]
   1093 
   1094 @_wraps(np.split, lax_description=_ARRAY_VIEW_DOC)

~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py in handler(signum, frame)
     64         # This following call uses `waitid` with WNOHANG from C side. Therefore,
     65         # Python can still get and update the process status successfully.
---> 66         _error_if_any_worker_fails()
     67         if previous_handler is not None:
     68             assert callable(previous_handler)

RuntimeError: DataLoader worker (pid 3197) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
RuntimeError: DataLoader worker (pid 3197) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
